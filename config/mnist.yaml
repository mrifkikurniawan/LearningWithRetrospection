trainer:
    module: LWR #Naive or LWR
    learning_rate: 0.001
    temperature: 5
    optimizer: SGD
    seed: 0
    gpus: 1
    logger: TensorBoardLogger
    max_epochs: 10
    k: 1
    metrics: [Accuracy, Precision, Recall, F1]

model:
    model_name: resnet18
    num_classes: 10
    checkpoint_path: ""
    pretrained: False
    in_chans: 1

loss: [{"module": CrossEntropyLoss,
        "weight": 1,
        "preds": preds,
        "targets": targets},
       {"module": KLDivLoss,
        "weight": 1,
        "preds": preds_T,
        "targets": soft_targets}]

datasets:
    name: MNIST
    train_val_ratio: 
        train: 0.8
        val: 0.2
    train:
        root: /media/user/DATA/my_repo/LearningWithRetrospection/datasets
        train: True
        download: True
        transform: "transforms.Compose([
                       transforms.ToTensor(),
                       transforms.Normalize((0.5), (0.5))])"
    val:
        transform: "transforms.Compose([
                       transforms.ToTensor(),
                       transforms.Normalize((0.5), (0.5))])"
    test:
        root: /media/user/DATA/my_repo/LearningWithRetrospection/datasets
        train: False
        download: True
        transform: "transforms.Compose([
                       transforms.ToTensor(),
                       transforms.Normalize((0.5), (0.5))])"

dataloaders:
    train:
        batch_size: 32
        num_workers: 4
        shuffle: True
    val:
        batch_size: 1
        num_workers: 4
        shuffle: False
    test:
        batch_size: 1
        num_workers: 4
        shuffle: False